{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "import hdbscan\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"02052022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2504\n",
      "[0.11746588 0.07753745 0.06567082 0.06018987 0.04714743 0.04224522\n",
      " 0.03319772 0.02928629 0.02678365 0.02518804 0.0223567  0.02002192\n",
      " 0.01902154 0.01640734 0.014639   0.01453726 0.01317917 0.01261853\n",
      " 0.0115835  0.01108979 0.01063509 0.01019487 0.0098358  0.00911172\n",
      " 0.00870613 0.00830372 0.00776264 0.0069973  0.00664834 0.00645602\n",
      " 0.00609701 0.00596179 0.00577315 0.00542573 0.00523902 0.00497274\n",
      " 0.00485709 0.00467923 0.00442287 0.00429822 0.00399582 0.00388305\n",
      " 0.00374099 0.00357532 0.0035578  0.00344443 0.0033413  0.00316093\n",
      " 0.00307428 0.00303906 0.00285026 0.00277067 0.00270431 0.00260208\n",
      " 0.00252568 0.00236006 0.00226772 0.00219271 0.00217047 0.00211034\n",
      " 0.00201059 0.00194082 0.00188631 0.00184289 0.00179477 0.00175591\n",
      " 0.0017307  0.00169461 0.00160777 0.00157497 0.00156791 0.00153241\n",
      " 0.00149444 0.00143675 0.00137922 0.00137012 0.00130598 0.00128881\n",
      " 0.00127984 0.0012629  0.00119334 0.00118314 0.00116113 0.00113325\n",
      " 0.00108968 0.00107718 0.00106255 0.00101848 0.00100943 0.00099206\n",
      " 0.00098505 0.00094974 0.00092552 0.0009006  0.00085569 0.00083779\n",
      " 0.00082649 0.00081657 0.000801   0.00079304]\n",
      "0.92728233\n",
      "[0.18716416 0.12244087 0.07831293 0.05038517 0.04084049 0.03746231\n",
      " 0.02704756 0.02512948 0.02394892 0.02232459 0.01749438 0.01698864\n",
      " 0.01657775 0.01555216 0.01385155 0.01351854 0.01231028 0.01185789\n",
      " 0.01136131 0.0102085  0.00947718 0.00881534 0.00820091 0.00770178\n",
      " 0.00715348 0.00669234 0.00647526 0.00624973 0.00592061 0.00563965\n",
      " 0.00536145 0.00532386 0.0052207  0.00492807 0.00457006 0.00446673\n",
      " 0.00435148 0.00406213 0.0039276  0.00373983]\n",
      "0.8730556605494684\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Tweets\" + date + \".csv\")\n",
    "df = df.loc[df['Retweeting'] == \"$\"]\n",
    "ce = np.load(date + \"_corpus_embeddings.npy\")\n",
    "he = np.load(date + \"_hashtag_embeddings.npy\")\n",
    "ce = ce[df['Hashtags'] != '[]']\n",
    "he = he[df['Hashtags'] != '[]']\n",
    "df = df.loc[df['Hashtags'] != '[]']\n",
    "print(len(df))\n",
    "df.index = np.arange(0, len(df))\n",
    "pca = PCA(n_components = 100)\n",
    "ce_pcad = pca.fit_transform(ce)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "pca = PCA(n_components = 40)\n",
    "he_pcad = pca.fit_transform(he)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "embeddings = np.concatenate([ce_pcad, he_pcad], axis = 1)\n",
    "#embeddings = ce_pcad\n",
    "#cluster = hdbscan.HDBSCAN(min_cluster_size = 12, metric = 'euclidean', cluster_selection_method='eom').fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "cluster = hdbscan.HDBSCAN(min_cluster_size = 8, metric = 'euclidean', cluster_selection_method='eom').fit(embeddings)\n",
    "print(np.unique(cluster.labels_)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20766006389776358574\n",
      "2504\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#Find all words in the topic\n",
    "unique_words = []\n",
    "word_occurunces = []\n",
    "num_topics = len(np.unique(cluster.labels_))\n",
    "\n",
    "for i in range(len(df['Content'])):\n",
    "    print(100*i/len(df['Content']), end = \"\\r\")\n",
    "    tweet = df['Content'][i]\n",
    "    label = cluster.labels_[i] + 1\n",
    "    for word in (re.split(\" |,|\\!|\\.|\\?|\\\"|\\n\",tweet)):\n",
    "        if word != \"\" and word != \"https://t\" and word[:3] != \"co/\":\n",
    "            if word in unique_words:\n",
    "                ind = unique_words.index(word)\n",
    "                word_occurunces[ind][label] = word_occurunces[ind][label] + 1\n",
    "            else:\n",
    "                unique_words.append(word)\n",
    "                word_occurunces.append(np.zeros(num_topics))\n",
    "                word_occurunces[-1][label] = 1\n",
    "                \n",
    "#relative topic frequency\n",
    "totals = []\n",
    "for i in range(len(word_occurunces)):\n",
    "    tot = word_occurunces[i].sum()\n",
    "    totals.append(tot)\n",
    "    word_occurunces[i] = word_occurunces[i] / tot\n",
    "totals = np.array(totals)\n",
    "unique_words = np.array(unique_words)\n",
    "word_occurunces = np.array(word_occurunces)\n",
    "\n",
    "unique_words_f = unique_words[totals > 4]\n",
    "word_occurunces_f = word_occurunces[totals > 4]\n",
    "print(len(unique_words_f))\n",
    "\n",
    "topic_predictors = []\n",
    "for label in np.unique(cluster.labels_):\n",
    "    topic_predictors.append(str(unique_words_f[np.argsort(word_occurunces_f[:,label+1])[-10:]]))\n",
    "    \n",
    "topic_predictors = np.array(topic_predictors)\n",
    "topic_predictors = topic_predictors[cluster.labels_ + 1]\n",
    "print(len(topic_predictors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 3)\n",
    "xyz = pca.fit_transform(embeddings)\n",
    "cluster_label = cluster.labels_\n",
    "content_with_br = []\n",
    "for tweet in df.Content:\n",
    "    i = 0\n",
    "    num_chars = 0\n",
    "    while (i < len(tweet)):\n",
    "        if tweet[i] == \" \" and num_chars > 30:\n",
    "            tweet = tweet[:i] + \"<br>\" + tweet[i:]\n",
    "            i += 4\n",
    "            num_chars = 0\n",
    "        i += 1\n",
    "        num_chars += 1\n",
    "    content_with_br.append(tweet)\n",
    "df[\"Content\"] = content_with_br\n",
    "df['x'] = xyz[:,0]\n",
    "df['y'] = xyz[:,1]\n",
    "df['z'] = xyz[:,2]\n",
    "df['cluster'] = cluster_label\n",
    "df['Topic Predictors'] = topic_predictors\n",
    "\n",
    "fig = px.scatter_3d(df, x = \"x\", y=\"y\", z=\"z\", hover_name = \"Name\", color=\"cluster\", hover_data = {\"x\": False, \"y\":False, \"z\":False, \"Content\":True, \"Topic Predictors\":True})\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "fig.update_layout(\n",
    "    scene = dict(\n",
    "        xaxis = dict(visible=False),\n",
    "        yaxis = dict(visible=False),\n",
    "        zaxis = dict(visible=False),\n",
    "    )\n",
    ")\n",
    "fig.write_html(\"test_hashtags.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11831037 0.09863859 0.05693945 0.04527569 0.04097782 0.03503473\n",
      " 0.0305344  0.02718597 0.02525349 0.02386177 0.02101102 0.02034496\n",
      " 0.01651567 0.0153907  0.01522642 0.0142732  0.01289163 0.01262383\n",
      " 0.01181986 0.011151   0.01033909 0.01030575 0.00942979 0.00919538\n",
      " 0.00889236 0.00792298 0.00755935 0.0074417  0.00699518 0.00671996\n",
      " 0.00637901 0.00634516 0.00612737 0.00594692 0.0057208  0.00560435\n",
      " 0.00530655 0.00487622 0.00464188 0.00449929 0.00448869 0.00417572\n",
      " 0.00405322 0.00380332 0.00370054 0.00359923 0.00344706 0.0033449\n",
      " 0.00321796 0.0031066  0.00304844 0.0029569  0.00283566 0.0026811\n",
      " 0.00262129 0.0025769  0.00247874 0.00236547 0.00229622 0.00219638\n",
      " 0.00214022 0.00210088 0.0020434  0.00200328 0.00194513 0.00191969\n",
      " 0.00185893 0.00184688 0.00179484 0.00173277 0.00167665 0.00162699\n",
      " 0.00161391 0.00153827 0.00152395 0.00151405 0.00144942 0.00140733\n",
      " 0.00137814 0.00134658 0.00131231 0.00130143 0.0012616  0.00124155\n",
      " 0.00121353 0.00118411 0.00115546 0.00113113 0.00112786 0.00108334\n",
      " 0.00105825 0.00105781 0.00103458 0.00100341 0.00098106 0.00096447\n",
      " 0.00095262 0.00093948 0.00092422 0.00090496 0.00087755 0.00087478\n",
      " 0.00085337 0.00083853 0.00082449 0.00081824 0.00081218 0.00078664\n",
      " 0.00076539 0.0007593  0.00074314 0.00073579 0.00071783 0.0007032\n",
      " 0.00069833 0.00069476 0.00067675 0.00067117 0.0006573  0.00065328\n",
      " 0.00063784 0.0006275  0.0006185  0.00061292 0.00060387 0.00059752\n",
      " 0.00057815 0.00057248 0.00057154 0.00056362 0.00055647 0.00054944\n",
      " 0.00054446 0.00053605 0.00052714 0.00052499 0.00051177 0.00050492\n",
      " 0.00049663 0.00048816 0.00048391 0.00047482 0.00046875 0.0004587\n",
      " 0.00045333 0.0004448  0.00044063 0.00043661 0.00043218 0.00042766\n",
      " 0.00042347 0.00042224 0.0004136  0.00041295 0.00041013 0.00040381\n",
      " 0.00039804 0.00039309 0.00038822 0.00038288 0.00038018 0.00037934\n",
      " 0.00037086 0.00036475 0.0003591  0.00035611 0.00035302 0.00035163\n",
      " 0.00034926 0.00034611 0.00034046 0.00033884 0.00033191 0.00033091\n",
      " 0.00032909 0.00032456 0.00031793 0.00031661 0.00031394 0.00031077\n",
      " 0.00030556 0.00030333 0.00030049 0.00029922 0.00029274 0.0002907\n",
      " 0.00028515 0.00028275 0.00027833 0.00027651 0.00027147 0.00027062\n",
      " 0.00026575 0.00026402 0.00026309 0.00026003 0.00025716 0.00025625\n",
      " 0.00025268 0.00024533]\n",
      "0.96014094\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Tweets\" + date + \".csv\")\n",
    "df = df.loc[df['Retweeting'] == \"$\"]\n",
    "ce = np.load(date + \"_corpus_embeddings.npy\")\n",
    "ce = ce[df['Hashtags'] == '[]']\n",
    "df = df.loc[df['Hashtags'] == '[]']\n",
    "df.index = np.arange(0, len(df))\n",
    "pca = PCA(n_components = 200)\n",
    "ce_pcad = pca.fit_transform(ce)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "embeddings = ce_pcad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "cluster = hdbscan.HDBSCAN(min_cluster_size = 8, metric = 'euclidean', cluster_selection_method='eom').fit(embeddings)\n",
    "print(np.unique(cluster.labels_)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44908502097064111646\n",
      "6676\n"
     ]
    }
   ],
   "source": [
    "#Find all words in the topic\n",
    "unique_words = []\n",
    "word_occurunces = []\n",
    "num_topics = len(np.unique(cluster.labels_))\n",
    "\n",
    "for i in range(len(df['Content'])):\n",
    "    print(100*i/len(df['Content']), end = \"\\r\")\n",
    "    tweet = df['Content'][i]\n",
    "    label = cluster.labels_[i] + 1\n",
    "    for word in (re.split(\" |,|\\!|\\.|\\?|\\\"|\\n\",tweet)):\n",
    "        if word != \"\" and word != \"https://t\" and word[:3] != \"co/\":\n",
    "            if word in unique_words:\n",
    "                ind = unique_words.index(word)\n",
    "                word_occurunces[ind][label] = word_occurunces[ind][label] + 1\n",
    "            else:\n",
    "                unique_words.append(word)\n",
    "                word_occurunces.append(np.zeros(num_topics))\n",
    "                word_occurunces[-1][label] = 1\n",
    "                \n",
    "#relative topic frequency\n",
    "totals = []\n",
    "for i in range(len(word_occurunces)):\n",
    "    tot = word_occurunces[i].sum()\n",
    "    totals.append(tot)\n",
    "    word_occurunces[i] = word_occurunces[i] / tot\n",
    "totals = np.array(totals)\n",
    "unique_words = np.array(unique_words)\n",
    "word_occurunces = np.array(word_occurunces)\n",
    "\n",
    "unique_words_f = unique_words[totals > 4]\n",
    "word_occurunces_f = word_occurunces[totals > 4]\n",
    "print(len(unique_words_f))\n",
    "\n",
    "topic_predictors = []\n",
    "for label in np.unique(cluster.labels_):\n",
    "    topic_predictors.append(str(unique_words_f[np.argsort(word_occurunces_f[:,label+1])[-10:]]))\n",
    "    \n",
    "topic_predictors = np.array(topic_predictors)\n",
    "topic_predictors = topic_predictors[cluster.labels_ + 1]\n",
    "print(len(topic_predictors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 3)\n",
    "xyz = pca.fit_transform(embeddings)\n",
    "cluster_label = cluster.labels_\n",
    "content_with_br = []\n",
    "for tweet in df.Content:\n",
    "    i = 0\n",
    "    num_chars = 0\n",
    "    while (i < len(tweet)):\n",
    "        if tweet[i] == \" \" and num_chars > 30:\n",
    "            tweet = tweet[:i] + \"<br>\" + tweet[i:]\n",
    "            i += 4\n",
    "            num_chars = 0\n",
    "        i += 1\n",
    "        num_chars += 1\n",
    "    content_with_br.append(tweet)\n",
    "df[\"Content\"] = content_with_br\n",
    "df['x'] = xyz[:,0]\n",
    "df['y'] = xyz[:,1]\n",
    "df['z'] = xyz[:,2]\n",
    "df['cluster'] = cluster_label\n",
    "df['Topic Predictors'] = topic_predictors\n",
    "\n",
    "fig = px.scatter_3d(df, x = \"x\", y=\"y\", z=\"z\", hover_name = \"Name\", color=\"cluster\", hover_data = {\"x\": False, \"y\":False, \"z\":False, \"Content\":True, \"Topic Predictors\":True})\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.update_traces(marker=dict(size=2))\n",
    "fig.update_layout(\n",
    "    scene = dict(\n",
    "        xaxis = dict(visible=False),\n",
    "        yaxis = dict(visible=False),\n",
    "        zaxis = dict(visible=False),\n",
    "    )\n",
    ")\n",
    "fig.write_html(\"test_no_hashtags.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
